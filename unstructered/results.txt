
First few results:
  participant  trial        word  surprisal   entropy  reading_time
0     l10_202      1     Leading   0.871952  5.415402           508
1     l10_202      1       water   3.657540  7.994276           534
2     l10_202      1  scientists   2.547961  7.353338           236
3     l10_202      1      issued   4.815400  4.638989           168
4     l10_202      1         one   2.816350  3.234753           287

Data ranges:
  Surprisal: 0.00 - 20.62
  Entropy: 0.02 - 35.40
  RT: 60.0 - 1225.0 ms

=== COMPREHENSIVE HYPOTHESIS TESTING (560644 observations) ===
MODEL PERFORMANCE:
  1. Surprisal only: RÂ² = 0.0143, p = 0.00e+00
  2. Entropy only:   RÂ² = 0.0095, p = 0.00e+00
  3. Combined:       RÂ² = 0.0175
     - Surprisal p = 0.00e+00
     - Entropy p   = 0.00e+00

ðŸ§ª KEY HYPOTHESIS TESTS WITH P-VALUES:

H1: Surprisal vs Entropy (which predicts reading times better?)
   â†’ Surprisal: RÂ² = 0.0143, p = 0.00e+00
   â†’ Entropy:   RÂ² = 0.0095, p = 0.00e+00
   â†’ SURPRISAL WINS: 50.4% better RÂ² than entropy
   â†’ Surprisal significant: YES (p = 0.00e+00)
   â†’ Entropy significant:   YES (p = 0.00e+00)

H2: Does entropy add predictive power beyond surprisal?
   â†’ RÂ² improvement: 0.0031 (22.0% better)
   â†’ F-test p-value: 0.00e+00
   â†’ Entropy coefficient p-value: 0.00e+00
   â†’ CONCLUSION: YES - Entropy adds significant value!
   â†’ Statistical evidence: F-test significant (p < 0.05)
   â†’ Practical evidence: 22.0% improvement in RÂ²

H3: Statistical significance of model comparisons
   â†’ Combined vs Surprisal-only:
     â€¢ F-test p-value: 0.00e+00
     â€¢ Significant: YES
   â†’ Combined vs Entropy-only:
     â€¢ F-test p-value: 0.00e+00
     â€¢ Significant: YES
   â†’ Direct correlation comparison:
     â€¢ Surprisal-RT: r = 0.120, p = 0.00e+00
     â€¢ Entropy-RT:   r = 0.098, p = 0.00e+00

EFFECT SIZE ANALYSIS:
   â†’ Cohen's fÂ² = 0.220 (MEDIUM effect)
   â†’ Interpretation:
     â€¢ Medium practical significance - entropy meaningfully improves prediction

ðŸ“Š STATISTICAL SUMMARY:
   â†’ Best single predictor: Surprisal
   â†’ Entropy adds significant value: YES
   â†’ Key p-values:
     â€¢ Surprisal significance: p = 0.00e+00
     â€¢ Entropy significance: p = 0.00e+00
     â€¢ Entropy in combined model: p = 0.00e+00
     â€¢ Model improvement F-test: p = 0.00e+00
âœ“ Plot saved as 'simple_entropy_analysis.png'

âœ“ Results saved to 'simple_test_results.csv'

=== FINAL SUMMARY ===
âœ… SUCCESS: Processed 560644 word observations from 9976 trials
   ðŸ“Š Model Performance:
      â€¢ Surprisal only: RÂ² = 0.0143
      â€¢ Entropy only:   RÂ² = 0.0095
      â€¢ Combined:       RÂ² = 0.0175

   ðŸ” KEY FINDINGS:
      â€¢ Surprisal is better single predictor than entropy
      â€¢ âœ… Entropy DOES add significant predictive power!
      â€¢ ðŸ“ˆ Improvement: +0.0031 RÂ² (22.0% better)
      â€¢ ðŸ“ Effect size: medium

   ðŸ“‹ COMPARISON TO TASK 1:
      â€¢ Task 1 Pythia RÂ²: 0.0130
      â€¢ Our Surprisal RÂ²: 0.0143
      â€¢ Relative effect: 1.1x stronger
      â€¢ ðŸ†• Entropy adds 0.31% more explained variance!
